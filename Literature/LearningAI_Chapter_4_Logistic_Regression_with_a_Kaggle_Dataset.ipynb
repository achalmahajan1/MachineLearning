{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LearningAI: Chapter 4 - Logistic Regression with a Kaggle Dataset.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this chapter, we will be looking at a real dataset which can be found on Kaggle. The dataset we’ll be using is the Pima Indians Diabetes datase.\n",
        "\n",
        "We’re going to import the csv file using Pandas. Pandas is a famous data science library used to hold data and analyze data. It holds the data in what is known as a dataframe.\n",
        "\n",
        "Source: https://medium.com/a-coders-guide-to-ai/a-coders-guide-to-neural-networks-chapter-4-pima-indians-diabetes-dataset-431c90be767f"
      ],
      "metadata": {
        "id": "O1dnm5Efx83r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "OM3860fByFuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's import the dataset"
      ],
      "metadata": {
        "id": "nTM4ujYryRbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(r'https://raw.githubusercontent.com/a-coders-guide-to-ai/a-coders-guide-to-neural-networks/master/data/diabetes.csv')"
      ],
      "metadata": {
        "id": "0cGVC7e_yUS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head() # Let's take a look at how the data looks like"
      ],
      "metadata": {
        "id": "kHatfy_Syavf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to previous chapters, we have 1 neuron and our loss function will remain the BCE. The only difference is, instead of 2 inputs (i.e. X and 1), we now have 9 inputs (i.e. all the features of the dataset and 1).\n",
        "\n",
        "Let's predict with Logistic Regression. We will split our dataset into 2 parts. 1 part holding the features and the other holding the target. Thereafter, we’ll turn those into tensors."
      ],
      "metadata": {
        "id": "ToZFngFW1wtO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[df.columns[:-1]] # takes all the columns of the dataset, except for the last column\n",
        "y = df['Outcome'] #  only takes the Outcome column and assigns it to y\n",
        "\n",
        "X = torch.tensor(X.values)\n",
        "y = torch.tensor(y.values)\n",
        "\n",
        "X = torch.cat((X, torch.ones(X.shape[0], dtype=float).reshape(-1,1)), dim=1)"
      ],
      "metadata": {
        "id": "mR7ECicU3fC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start writing the regression code now taking motivation from the previous chapter\n"
      ],
      "metadata": {
        "id": "v7-9o8OW7zuZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_loss(y, y_pred): ##log loss error (binary cross entropy)\n",
        "  return -torch.sum((y*torch.log(y_pred) + (1-y)*torch.log(1-y_pred)))/y.shape[0]\n",
        "\n",
        "def sigmoid(linear):\n",
        "    return 1/(1+torch.exp(-linear))\n",
        "\n",
        "epoch_loss = []\n",
        "\n",
        "weights = torch.zeros(9, dtype=float, requires_grad=True)\n",
        "learning_rate = 1e-4\n",
        "n = X.shape[0]\n",
        "\n",
        "for epoch in range(8000+1): \n",
        "    linear = weights.reshape(1,-1)@X.T ##one equation for all\n",
        "    print(weights.reshape(1,-1))\n",
        "    print(weights)\n",
        "\n",
        "    y_pred =  sigmoid(linear) ##logistic\n",
        "    loss = log_loss(y, y_pred)\n",
        "    epoch_loss.append(loss.item())\n",
        "\n",
        "    ###backpropagation###\n",
        "    loss.backward()\n",
        "\n",
        "    ###step###\n",
        "    with torch.no_grad():\n",
        "        weights -= learning_rate * weights.grad\n",
        "\n",
        "    weights.grad.zero_()\n",
        "\n",
        "    if(epoch % 1000 == 0):\n",
        "        acc = torch.sum(((y_pred>=0.5)+0 == y)+0).item()/y.shape[0]\n",
        "        print('epoch: {0} - loss: {1:.5f}; acc: {2:.3f}'.format(epoch, epoch_loss[-1], acc))"
      ],
      "metadata": {
        "id": "bvGifbqR75T-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample to check how reshape works\n",
        "import numpy as np\n",
        "a = np.array([[1,2,3], [4,5,6]])\n",
        "print(a)\n",
        "print(np.reshape(a, (1,-1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSpyZzZP-1Fk",
        "outputId": "d8179c70-abb4-4f14-8176-c1aef520a59f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 2 3]\n",
            " [4 5 6]]\n",
            "[[1 2 3 4 5 6]]\n"
          ]
        }
      ]
    }
  ]
}